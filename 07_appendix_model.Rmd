# Appendices {#Appmethods}

## Spatiotemporal Solute Concentration Smoother {#SSCS}

The spatiotemporal solute concentration smoother is estimated using a non parametric
regression technique known as Penalised Splines (P-Splines). It is beyond the scope of this document to give a full and detailed explanation of this technique here. However, the
following outlines some of the most important aspects for the purposes of GWSDAT. For a more detailed explanation the reader is referred to @Eilers92 and @Eilers96.

Let $y_i$ be the solute concentration at $\boldsymbol{x_i}  =(x_{i1}, x_{i2}, x_{i3})$
where $x_{i1}$ and $x_{i2}$ stand for the spatial coordinates of the well and $x_{i3}$ represents the corresponding time point for the *i-th* observation with $i = 1, \ldots, n$. We start by modelling the solute concentration as

\begin{equation}
      y_i = \sum_{j=1}^m\, b_j(\boldsymbol{x_i})\alpha_j + \epsilon_i
\label{eq:int}
\end{equation}
where the $b_j$, $j = 1, \ldots, m$ are $m$ functions (known as *basis functions*) conveniently chosen to achieve smoothness (generally a particular kind of polynomial of order 3). The first term in equation (\ref{eq:int}) is a linear combination of the basis functions $b_j$, each evaluated at $\boldsymbol{x_i}$, and aims at capturing the deterministic part of the \textit{i-th} observation, generally known as 'signal'; the second term, $\epsilon_i$, accounts for the variability in the measurement due to randomness and is usually termed as 'noise'. The behaviour of $\epsilon_i$ is described in terms of a convenient probabilistic model; such a model guarantees that the value of $\epsilon_i$ fluctuates around zero conveying the idea that we do not expect to make any systematic error in the measurement. This model also comprises the notion that the expected spread of $\epsilon_i$ is given by $\sigma^2$,
%a non-negative parameter $\sigma$; its squared
%value  is known as
the *variance* of the random component $\epsilon_i$.  By using the matrix notation

\begin{equation*}
\boldsymbol B(\boldsymbol{x}) =
 \begin{pmatrix}
  b_1(x_1)   & \cdots &  b_j(x_1)  & \cdots &  b_m(x_1) \\
  b_1(x_2)   & \cdots &  b_j(x_2)  & \cdots &  b_m(x_2) \\
  \vdots     & \cdots &  \vdots    & \cdots &  \vdots   \\
  b_1(x_i)   & \cdots &  b_j(x_i)  & \cdots &  b_m(x_i) \\
  \vdots     & \cdots &  \vdots    & \cdots &  \vdots   \\
  b_1(x_n)   & \cdots &  b_j(x_n)  & \cdots &  b_m(x_n) \\
\end{pmatrix}
\end{equation*}

equation (\ref{eq:int}) can be written in a more compact fashion as
$\boldsymbol y  = \boldsymbol B(\boldsymbol{x})\boldsymbol\alpha + \boldsymbol\epsilon$. Because, as mentioned earlier, we expect the $\epsilon_i$'s to oscillate around zero, a sensible choice for the regression parameters $\boldsymbol\alpha$ is the one that minimises the norm of the vector $\boldsymbol\epsilon$ defined as $\mathrm{S}\left(\boldsymbol\alpha\right) = \Vert
\boldsymbol\epsilon \Vert^2 = \Vert \boldsymbol y - \boldsymbol B(\boldsymbol x) \boldsymbol\alpha\Vert^2$. A large value of basis functions is
generally chosen to allow the model to capture most of the signal. The downside of this approach is that it tends also to overfit, that is to fit the noise in the observations, with the consequent loss of smoothness. To overcome this hurdle, the objective function
%to be optimised $\mathrm{S}\left(\boldsymbol\alpha\right)$ is
modified with the addition of a term that penalises the lack of smoothness of the fit.

The objective function now takes the form
$\mathrm{S}\left(\boldsymbol\alpha\right)
  = \Vert \boldsymbol y - \boldsymbol B(\boldsymbol x) \boldsymbol\alpha\Vert^2 + \lambda \Vert D \alpha \Vert^2$
where $\lambda$ is a non-negative smoothing parameter and $D$ is the $(m-2) \times m$ matrix

\begin{equation*}
\boldsymbol D = \begin{pmatrix}
      1 & -2  &  1 &  0 & 0 & \cdots & 0 & 0 & 0 \\
      0 &  1  & -2 &  1 & 0 & \cdots & 0 & 0 & 0 \\
      0 &  0  &  1 & -2 & 1 & \cdots & 0 & 0 & 0 \\
      \vdots &  \vdots  &  \vdots & \vdots & \vdots & \ddots  & \vdots \\
      0 &  0  &  0 & 0 & 0 & \cdots & 1  &-2 & 1
      \end{pmatrix}
\end{equation*}

The additional term in the objective function

$$\Vert D \alpha \Vert^2   =  \left(\alpha_1 - 2\alpha_2 + \alpha_3\right)^2 +
                                                  \ldots +
                                                \left(\alpha_{m-2} - 2\alpha_{m-1} + \alpha_m\right)^2
$$

controls the smoothness of the fit by applying penalties over adjacent coefficients.
By minimising the new objective function for a given value of $\lambda$, we obtain
the least squares estimator of the parameters
$$
      \boldsymbol{\hat{\alpha}} = \left(\boldsymbol B'\boldsymbol B + \lambda \boldsymbol D'\boldsymbol D\right)^{-1}\boldsymbol B'\boldsymbol y .
$$
Consequently, the fitted values are given by:
$$
\boldsymbol{\hat{y}} = \boldsymbol{B\hat{\alpha}} =
\boldsymbol B\left(\boldsymbol B'\boldsymbol B + \lambda \boldsymbol D' \boldsymbol D\right)^{-1}\boldsymbol B'\boldsymbol y = \boldsymbol{Hy}
$$

When $\lambda = 0$, the expression for the estimator of the parameters {\mathversion{bold} $\hat{\alpha}$ } boils down to the classical solution in linear models theory. As $\lambda \rightarrow \infty$, the fitted function tends to a linear function. The Figure below shows the effect of penalisation: it forces the coefficients to yield a smooth pattern. The fitting process of a function using B-Splines is pictured with and without penalisation, together with the basis functions (the columns of the $\boldsymbol B$ matrix). The left plot
 results from not penalising ($\lambda=0$) the term in the objective function that accounts for the smoothness; it can be noticed that it yields a rather wiggly regression function. In the right plot, a suitable choice for $\lambda$ constrains the optimisation method to find values for the coefficients $\hat{\alpha}$ which
result in a smoother regression curve.

```{r echo = FALSE, out.width = '50%', fig.align = 'default', fig.show = 'hold'}
include_graphics(c("figures/PSplines_00.jpeg", "figures/PSplines_06.jpeg"))
```

fig.cap: Curve based on 20 nodes in the basis, without penalisation (left), with penalisation (right).

Prior to fitting the regression coefficients $\boldsymbol \alpha$ the observed solute concentration values are natural log transformed. This avoids the possibility of predicting negative concentration values and also helps the model cope with data which often spans several orders of magnitude. Furthermore, the uncertainty in the measured concentrations can reasonably be expected to be proportional to the magnitude of the value, e.g. the uncertainty around a measured value of 10ug/l would be expected to be very much less than the uncertainty surrounding a measured value of 10000ug/l. The natural log transformation stabilises the variance.

The choice of the penalisation parameter $\lambda$ is a crucial matter as a
too small value would result in 'overfitting' (tracking the noise) whereas an extremely large value would lead to 'underfitting' (producing a flat estimated function as a result of loss of signal). Several criteria have been proposed, such
as those described by @Hurvich and @WoodB, but we tackled the issue by a *Bayesian* approach; see @Denison, @Raftery and @WoodP.

Under this paradigm, $\lambda$ is not considered to be a fixed unknown quantity to be estimated but rather a random variable whose value may vary within a given range. This behaviour is described in probabilistic terms which assign a measure of confidence or *probability* to each of the values $\lambda$ may take on.

The Bayesian framework allows to compute the probability that the random variable
$\lambda$ may take a particular value, conditional on the fact that
$\boldsymbol y$ has already been observed. This probability, indicated as
$f(\lambda | \boldsymbol y)$, is known as the *posterior distribution* of $\lambda$.

Bayes' rule states that $f(\lambda | \boldsymbol y) \propto f(\boldsymbol y | \lambda) f(\lambda)$ where $\propto$ stands for "proportional to". $f(\boldsymbol y | \lambda)$ is known as the *likelihood function* and expresses the conditional probability of observing data $\boldsymbol y$, given that the true value of the parameter is $\lambda$; $f(\lambda)$ is known as the *prior distribution* of the random variable $\lambda$ and comprises our prior beliefs on its uncertainty.

The optimal value of $\lambda$ is the one that maximises the posterior distribution and is computed using numerical methods.




## Plume Diagnostics {#AppendixPlumeDiagnostics}
GWSDAT calculates plume diagnostic quantities from the predictions of the [`Spatiotemporal Solute Concentration Smoother`](#SSCS). In common to @Maros1 and @RickerPlumeQuant, numerical methods are employed to integrate out the plume diagnostic quantities. For a given model time step a fine spatial mesh grid of predictions is generated. The plume boundary region $D$, for a given plume threshold concentration value, is calculated using the R function \emph{contourLines} which is included in the base distribution of the R programming language (@R).
The plume area, $A$, is defined as 
<!-- %ref to Ricker. -->
\begin{equation}\label{EqPlumeArea}
A=\iint\limits_D dx dy
\end{equation}

where where $x$ and $y$ are the spatial coordinates and is calculated numerically using the \emph{areapl} function from the R package splancs @splancs. 
The average plume concentration is defined as 
\begin{equation}\label{EqPlumeAveragePlumeConc}
\frac{1}{A}\iint\limits_D  \hat{s_t}\left(x,y\right)  dx dy
\end{equation}
where $\hat{s_t}\left(x,y\right)$ represents the predictions of the spatiotemporal solute concentration smoother evaluated at time $t$. 
This integral (and all subsequent integrals in this section) is calculated numerically using a method described in @Oloufa91. 
A Delaunay triangulation is performed using the R package 'deldir', @deldir, on the spatial mesh grid of predictions within the plume boundary, $D$.  
The integral is numerically approximated by summing up the individual volumes under each prism formed. 

Plume mass is calculated from the scaled product of plume area and average concentration. The scaling factor encompasses the user specified value of ground porosity (see [`Plume Diagnostics`](#GWSDATPlumeDiagnostics)) and appropriate scaling values for mapping together the volumetric concentration units (e.g. ug/l) with the length scale (see \textbf{CoordUnits} in [`Well Coordinates Table`](#WellCoordinatesTable)) of the Well coordinates (e.g. metres or feet). 
The plume mass is calculated on a mass per unit aquifer depth basis (e.g. kg/m). To calculate the total plume mass the user must multiply this value by the aquifer depth.


The plume center of mass $(x,y)$ is defined as the mean location of the concentration distribution within the plume boundary region $D$. The x-coordinate of the plume Centre of Mass is evaluated by numerical calculation of 

\begin{equation}\label{EqCOMX}
X_c= \frac{\iint\limits_D x \hat{s_t}\left(x,y\right)  dx dy}{\iint\limits_D  \hat{s_t}\left(x,y\right)  dx dy}
\end{equation}

where $x$ and $y$ are the spatial coordinates and $\hat{s_t}\left(x,y\right)$ represents the predictions of the spatiotemporal solute concentration smoother evaluated at time $t$. In a completely analogous manner the y-coordinate of the plume Centre of Mass is evaluated as follows: 

\begin{equation}\label{EqCOMY}
Y_c= \frac{\iint\limits_D y \hat{s_t}\left(x,y\right)  dx dy}{\iint\limits_D  \hat{s_t}\left(x,y\right)  dx dy}
\end{equation}

In the event that multiple plumes are detected then the above quantities are calculated for each individual plume and aggregated together. The individual plume areas and masses are summed to calculate the total over all plumes. The aggregate average plume concentration and aggregate plume centre of mass is calculated by taking a weighted average of the individual quantities. 






## Well Influence Analysis {#WellOrderDiagnostics}

This order is established via an approximation methodology documented [here](https://github.com/peterradv/Well-Influence-Analysis) 
Cook's distance @cook, devised by Ralph Dennis Cook in 1977, is commonly used to estimate the influence of a data point in least-squares regression analysis. It is a measure of the change in regression estimates if an observation is deleted. In a P-splines model, it can be expressed using standardised residuals, the effective degrees of freedom and leverages:


\begin{equation}
    CD = \frac{1}{p} (r_i^s)^2 \frac{h_{ii}}{1-h_{ii}},
\end{equation}

where $p$ is the effective degrees of freedom which is given by the trace of the hat matrix, $r^s_i$ is the internally studentised (standardised) residual of the $i$-th observation and $h_{ii}$ is the leverage of the fitted value as given by the $i$-th row and $i$-th column of the hat matrix $H$.
 



## Groundwater Flow Calculation {#GWCalc}
For a given model output interval the Groundwater (GW) flow strength and direction are estimated using 
available GW level and well coordinates data. The model is based on the simple premise that local 
GW flow will follow the local direction of steepest descent (hydraulic gradient). 

For a given well, a linear plane is fitted to the local GW level data:

\begin{equation}\label{GWLinSurface}
L_i=a+bx_i+cy_i+\epsilon_i
\end{equation} 
where $L_i$ represents the GW level at location $(x_i,y_i)$. Local data is defined as the neighbouring wells 
as given by a Delauney triangulation (\url{http://en.wikipedia.org/wiki/Delaunay_triangulation}, @DelRef) 
of the monitoring well locations.
The gradient of this linear surface in both x and y directions 
is given by the coefficients $b$ and $c$. Estimated direction of flow is given by: 

\begin{equation}
\theta = \tan^{-1}\left (\frac{c}{b} \right)
\end{equation} 
and the relative hydraulic gradient (a measure of relative flow velocity) is given by 

\begin{equation}
R=\sqrt{b^2+c^2}
\end{equation} 

For any given model output interval this algorithm is applied to each and every well where a GW level has been recorded. 





## Time Series Plot Smoother {#smregressCalc}

The time series plot smoother is fitted using a nonparametric method called local linear regression. 
This involves solving locally the least squares problem:

\begin{equation}\label{locallinreg}
\textrm{min}_{\alpha , \beta} \sum_{i}^{n}\left\{ y_i - \alpha - \beta (x_i-x) \right\}^2 w(x_i-x; h)
\end{equation} 

where $w(x_i-x; h)$ is called the kernel function. 
A normally-distributed probability density function with standard deviation $h$ is used as the kernel. 
$h$ is also called smoothing parameter that controls the width of the kernel function, 
and hence the degree of smoothing applied to the data (the higher the value of h, the smoother the estimates).
Within GWSDAT, local linear regression is deployed using the R package 'sm' (@smbook) and the
bandwidth is selected using the method published in @Hurvichetal. 














\newpage
## Converting a CAD drawing to a Shapefile {#ConvShapRosie}

System requirements: ArcGIS comprising ArcMap, ArcEditor, ArcCatalog

<!-- \begin{enumerate} -->
<!-- \item Open ArcCatalog from the Start Menu (`Start' -$>$ `All Programs' -$>$ `ArcGIS' -$>$ `ArcCatalog') -->
<!-- \item In ArcCatalog navigate to ArcMap (globe with magnifying glass icon) -->
<!-- \item 	When ArcMap opens a screen will pop-up. Select `A New Empty Map' then click `OK'. -->
<!-- \item 	 Go to `File' -$>$ `Add Data' (positive sign with yellow triangle underneath) -$>$ Select site CAD drawing saved as a `.dxf' file  -$>$ `Add'. -->
<!-- \item 	Click on the `+' symbol to expand the sub-layers of the dxf file (e.g. `Polyline', `Polygon', `Multipatch', `Point').  -->
<!-- \item 	Right click on required layer (e.g. Polyline or an edited \& exported shapefile) to open the drop down menu. -->
<!-- \item 	On drop down menu select `Data' -$>$ `Export Data'. -->
<!-- \item 	On `Export Data' pop-up menu choose `Select All Features' + `This Layers Source Data' and select the folder you wish to save the shapefile into, then click `OK'. -->
<!-- \item 	Click `Yes' to add the exported data as a new layer. -->
<!-- \item 	Repeat steps 6-9 to convert all the layers required to produce the base-map in GWSDAT into shapefiles. -->
<!-- \item 	Add the shapefiles into GWSDAT one by one to produce the complete base-map image. -->
<!-- \end{enumerate} -->

 #. Open ArcCatalog from the Start Menu ('Start' -$>$ 'All Programs' -$>$ 'ArcGIS' -$>$ 'ArcCatalog').
 
 #. In ArcCatalog navigate to ArcMap (globe with magnifying glass icon).
 
 #. When ArcMap opens a screen will pop-up. Select 'A New Empty Map' then click 'OK'.
 
 #. Go to 'File' -$>$ 'Add Data' (positive sign with yellow triangle underneath) -$>$ Select site CAD drawing saved as a '.dxf' file  -$>$ 'Add'.
 
 #. Click on the '+' symbol to expand the sub-layers of the dxf file (e.g. 'Polyline', 'Polygon', 'Multipatch', 'Point'). 
 
 #. Right click on required layer (e.g. Polyline or an edited \& exported shapefile) to open the drop down menu.
 
 #. On drop down menu select 'Data' -$>$ 'Export Data'.
 
 #. On 'Export Data' pop-up menu choose 'Select All Features' + 'This Layers Source Data' and select the folder you wish to save the shapefile into, then click 'OK'.
 
 #. Click 'Yes' to add the exported data as a new layer.
 
 #. Repeat steps 6-9 to convert all the layers required to produce the base-map in GWSDAT into shapefiles.
 
 #. Add the shapefiles into GWSDAT (see [`GIS ShapeFiles Table`](#ShapeFileTableExplan)) one by one to produce the complete base-map image. 


The next section details how to edit layers in ArcMap after their conversion to shapefiles, prior to upload into GWSDAT (useful for removing gridlines etc)

 #. Uncheck the CAD layer used to produce shapefile to remove image from view window.
 
 #.	Ensure exported shapefile is selected and visible in view window.
 
 #.	Click 'Start Editing' on the 'Editor' toolbar above the map.
 
 #.	Use the arrow pointer to select lines and press delete on the keyboard to remove from drawing. (Select 'UnDo' from 'Edit' Toolbar in case of errors).
 
 #.	'Editor' -$>$ 'Stop Editing'. Click 'Yes' to save edits.
 
 #.	Repeat data export as detailed in steps 6-9 above and re-save as new shapefile.













